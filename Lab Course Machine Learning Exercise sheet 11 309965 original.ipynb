{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPmOHkx4xDi4"
   },
   "source": [
    "# Lab Course Machine Learning\n",
    "## Exercise Sheet 11\n",
    "##### January, 2022  \n",
    "##### Kenechukwu Ejimofor\n",
    "###### Data Analytics\n",
    "<center>\n",
    "<b>\n",
    "Information Systems and Machine Learning Lab<br>\n",
    "University of Hildesheim<br>\n",
    "</b>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Universit%C3%A4t_Hildesheim_logo.svg/1200px-Universit%C3%A4t_Hildesheim_logo.svg.png\" height=\"10%\" width=\"10%\">\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUBvp7VKxXiP"
   },
   "source": [
    "#### Exercise 0: Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZpItuM4sw6SO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Kenechukwu\n",
      "[nltk_data]     Ejimofor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix #for sparse matrix\n",
    "import warnings\n",
    "\n",
    "np.random.seed(3116)\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "52U4BX_SxgrI"
   },
   "outputs": [],
   "source": [
    "category = [ 'sci.med', 'comp.graphics']\n",
    "dataset =  fetch_20newsgroups(subset='train',categories=category,random_state=3116)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3l6NmlFsZYs"
   },
   "source": [
    "Preview of an example in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdARW43HsNSI",
    "outputId": "9ef0c48f-54a4-4112-d010-3cf987016681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: kaminski@netcom.com (Peter Kaminski)\n",
      "Subject: Re: Krillean Photography\n",
      "Lines: 101\n",
      "Organization: The Information Deli - via Netcom / San Jose, California\n",
      "\n",
      "[Newsgroups: m.h.a added, followups set to most appropriate groups.]\n",
      "\n",
      "In <1993Apr19.205615.1013@unlv.edu> todamhyp@charles.unlv.edu (Brian M.\n",
      "Huey) writes:\n",
      "\n",
      ">I am looking for any information/supplies that will allow\n",
      ">do-it-yourselfers to take Krillean Pictures.\n",
      "\n",
      "(It's \"Kirlian\".  \"Krillean\" pictures are portraits of tiny shrimp. :)\n",
      "\n",
      "[...]\n",
      "\n",
      ">One might extrapolate here and say that this proves that every object\n",
      ">within the universe (as we know it) has its own energy signature.\n",
      "\n",
      "I think it's safe to say that anything that's not at 0 degrees Kelvin\n",
      "will have its own \"energy signature\" -- the interesting questions are\n",
      "what kind of energy, and what it signifies.\n",
      "\n",
      "I'd check places like Edmund Scientific (are they still in business?) --\n",
      "or I wonder if you can find ex-Soviet Union equipment for sale somewhere\n",
      "in the relcom.* hierarchy.\n",
      "\n",
      "Some expansion on Kirlian photography:\n",
      "\n",
      "From the credulous side: [Stanway, Andrew, _Alternative Medicine: A Guide\n",
      "To Natural Therapies_, ISBN 0-14-008561-0, New York: Viking Penguin, 1986,\n",
      "p211, p188.  A not-overly critical but still useful overview of 32\n",
      "alternative health therapies.]\n",
      "\n",
      "  ...the Russian engineer Semyon Kirlian and his wife Valentina during the\n",
      "  1950s.  Using alternating currents of high frequency to 'illuminate'\n",
      "  their subjects, they photographed them.  They found that if an object\n",
      "  was a good conductor (such as a metal) the picture showed only its\n",
      "  surface, while the pictures of poor conductors showed the inner\n",
      "  structure of the object even if it were optically opaque.  They found\n",
      "  too that these high frequency pictures could distinguish between dead\n",
      "  and living objects.  Dead ones had a constant outline whilst living ones\n",
      "  were subject to changes.  The object's life activity was also visible in\n",
      "  highly variable colour patterns.\n",
      "\n",
      "  High frequency photography has now been practised for twenty years in\n",
      "  the Soviet Union but only a few people in the West have taken it up\n",
      "  seriously.  Professor Douglas Dean in New York and Professor Philips at\n",
      "  Washington University in St Louis have produced Kirlian photographs and\n",
      "  others have been produced in Brazil, Austria and Germany.\n",
      "\n",
      "  Using Kirlian photography it is possible to show an aura around people's\n",
      "  fingers, notably around those of healers who are concentrating on\n",
      "  healing someone.  Normally, blue and white rays emanate from the fingers\n",
      "  but, when a subject becomes angry or excited, the aura turns red and\n",
      "  spotty.  The Soviets are now using Kirlian photography to diagnose\n",
      "  diseases which cannot be diagnosed by any other method.  They argue that\n",
      "  in most illnesses there is a preclinical stage during which the person\n",
      "  isn't actually ill but is about to be.  They claim to be able to\n",
      "  foretell a disease by photographing its preclinical phase.\n",
      "\n",
      "  But the most exciting phenomenon illustrated by Kirlian photography is\n",
      "  the phantom effect.  During high frequency photography of a leaf from\n",
      "  which a part had been cut, the photograph gave a complete picture of the\n",
      "  leaf with the removed part showing up faintly.  This is extremely\n",
      "  important because it backs up the experiences of psychics who can 'see'\n",
      "  the legs of amputees as if they were still there.  The important thing\n",
      "  about the Kirlian phantoms though is that the electromagnetic pattern\n",
      "  can't possibly represent a secondary phenomenon -- or the field would\n",
      "  vanish when the piece of leaf or leg vanished.  The energy grid\n",
      "  contained in a living object must therefore be far more significant than\n",
      "  the actual object itself.\n",
      "\n",
      "  [...]\n",
      "\n",
      "  Kirlian photography has shown how water mentally 'charged' by a healer\n",
      "  has a much richer energy field around it than ordinary water...\n",
      "\n",
      "\n",
      "From the incredulous side: [MacRobert, Alan, \"Reality shopping; a\n",
      "consumer's guide to new age hokum.\", _Whole Earth Review_, Autumn 1986,\n",
      "vNON4 p4(11).  An excellent article providing common-sense guidelines for\n",
      "evaluating paranormal claims, and some of the author's favorite examples\n",
      "of hokum.]\n",
      "\n",
      "  The crank usually works in isolation from everyone else in his field of\n",
      "  study, making grand discoveries in his basement.  Many paranormal\n",
      "  movements can be traced back to such people -- Kirlian photography, for\n",
      "  instance.  If you pump high-voltage electricity into anything it will\n",
      "  emit glowing sparks, common knowledge to electrical workers and\n",
      "  hobbyists for a century.  It took a lone basement crank to declare that\n",
      "  the sparks represent some sort of spiritual aura.  In fact, Kirlian\n",
      "  photography was subjected to rigorous testing by physicists John O.\n",
      "  Pehek, Harry J. Kyler, and David L. Faust, who reported their findings\n",
      "  in the October 15, 1976, issue of Science.  Their conclusion: The\n",
      "  variations observed in Kirlian photographs are due solely to moisture on\n",
      "  the surface of the body and not to mysterious \"auras\" or even\n",
      "  necessarily to changes in mood or mental state.  Nevertheless,\n",
      "  television shows, magazines, and books (many by famous\n",
      "  parapsychologists) continue to promote Kirlian photography as proof of\n",
      "  the unknown.\n",
      "\n",
      "-- \n",
      "Peter Kaminski\n",
      "kaminski@netcom.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pqUArbZsmBq",
    "outputId": "5ebcec13-79c1-45ea-903b-17d00abfeb22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: kaminski@netcom.com (Peter Kaminski)\n",
      "Subject: Re: Krillean Photography\n",
      "Lines: 101\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(dataset.data[0].split(\"\\n\")[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJPcEqPjh8Cz",
    "outputId": "5131c0fe-a94e-4a2d-8c96-2dbefb0a63b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "#Example classification\n",
    "print(dataset.target_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hh1YX76lssFr"
   },
   "source": [
    "### Preprocessing textual data to remove punctuation, stop-words \n",
    "- I decided to stem the words in other to reduce the maximum size in the count vectorizer during Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iDxjXDQPxtSF",
    "outputId": "8fb62d19-7c28-419d-ddda-bf10f00ab625"
   },
   "outputs": [],
   "source": [
    "corpus = [] #list to get processed data\n",
    "\n",
    "'''We remove all the punctuations by replacing them with spaces\n",
    "we do this by looping through the data and replacing any element\n",
    "that is not in the alphabet both upper and lower cases then convert the\n",
    "whole data to lower case'''\n",
    "\n",
    "for i in range(0, len(dataset.data)):\n",
    "    news = re.sub('[^a-zA-Z]',' ', dataset.data[i]) \n",
    "    news = news.lower()\n",
    "    news = news.split()\n",
    "    stemmer = PorterStemmer() #stemming class\n",
    "    news = [stemmer.stem(j) for j in news if not j in set(stopwords.words('english'))] \n",
    "  #We loop through the data to only input words absent from the stopwords\n",
    "    news = ' '.join(news) #join words and add space\n",
    "    corpus.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvZllaVUCZLK",
    "outputId": "71850a6b-8b39-4107-950f-c121bb9ddde2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1178"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "yWPWPcDPCfIz",
    "outputId": "72c8cdc3-d2d0-4d81-965f-928140c6954d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kaminski netcom com peter kaminski subject krillean photographi line organ inform deli via netcom san jose california newsgroup h ad followup set appropri group apr unlv edu todamhyp charl unlv edu brian huey write look inform suppli allow yourself take krillean pictur kirlian krillean pictur portrait tini shrimp one might extrapol say prove everi object within univers know energi signatur think safe say anyth degre kelvin energi signatur interest question kind energi signifi check place like edmund scientif still busi wonder find ex soviet union equip sale somewher relcom hierarchi expans kirlian photographi credul side stanway andrew altern medicin guid natur therapi isbn new york vike penguin p p overli critic still use overview altern health therapi russian engin semyon kirlian wife valentina use altern current high frequenc illumin subject photograph found object good conductor metal pictur show surfac pictur poor conductor show inner structur object even optic opaqu found high frequenc pictur could distinguish dead live object dead one constant outlin whilst live one subject chang object life activ also visibl highli variabl colour pattern high frequenc photographi practis twenti year soviet union peopl west taken serious professor dougla dean new york professor philip washington univers st loui produc kirlian photograph other produc brazil austria germani use kirlian photographi possibl show aura around peopl finger notabl around healer concentr heal someon normal blue white ray eman finger subject becom angri excit aura turn red spotti soviet use kirlian photographi diagnos diseas cannot diagnos method argu ill preclin stage person actual ill claim abl foretel diseas photograph preclin phase excit phenomenon illustr kirlian photographi phantom effect high frequenc photographi leaf part cut photograph gave complet pictur leaf remov part show faintli extrem import back experi psychic see leg ampute still import thing kirlian phantom though electromagnet pattern possibl repres secondari phenomenon field would vanish piec leaf leg vanish energi grid contain live object must therefor far signific actual object kirlian photographi shown water mental charg healer much richer energi field around ordinari water incredul side macrobert alan realiti shop consum guid new age hokum whole earth review autumn vnon p excel articl provid common sens guidelin evalu paranorm claim author favorit exampl hokum crank usual work isol everyon els field studi make grand discoveri basement mani paranorm movement trace back peopl kirlian photographi instanc pump high voltag electr anyth emit glow spark common knowledg electr worker hobbyist centuri took lone basement crank declar spark repres sort spiritu aura fact kirlian photographi subject rigor test physicist john pehek harri j kyler david l faust report find octob issu scienc conclus variat observ kirlian photograph due sole moistur surfac bodi mysteri aura even necessarili chang mood mental state nevertheless televis show magazin book mani famou parapsychologist continu promot kirlian photographi proof unknown peter kaminski kaminski netcom com'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview data\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TnCifKmDrqX"
   },
   "source": [
    "- Bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the bag of words model we need to implement a count vectorizer((For Tokenization). The following steps are required to implement the count vectorizer:\n",
    "- Get the unique words and set an index to each of these words\n",
    "- Iterate through each sentence and get the number of occurence for each unique word\n",
    "- Create a sparse matrix that represents all the available words(i.e size = number of unique word) and fill in the matrix with the respective counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that returns the unique words and their positions in a text\n",
    "def word_index(s):\n",
    "    unique_words = set() #Define a set of unique words to avoid duplicates\n",
    "    for sentence in s:\n",
    "        for word in sentence.split(' '):\n",
    "            if word not in stopwords.words('english'): #Exclude stopwords\n",
    "                unique_words.add(word)\n",
    "    #Now we need to return the words and the index\n",
    "    word_dict = {}\n",
    "    for idx, word in enumerate(sorted(list(unique_words))):\n",
    "        word_dict[word] = idx\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 0, 'eleven': 1, 'example': 2, 'lab': 3, 'vectorizer': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check \n",
    "s = ['this is lab eleven', 'here is the count vectorizer', 'this is an example'] #example for testing\n",
    "word_index(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice the stopwords `'this'`, `'is'`, `'an'`, `'here'` were excluded. Furthermore, the number of unique words is 5, therefore the Count Vector would consist of 3 lists with a length of 5 on each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountVectorizer(data):\n",
    "    #First step is the unique word indexing in the given input\n",
    "    dict_word = word_index(data)\n",
    "    row = []\n",
    "    column = []\n",
    "    value = []\n",
    "    \n",
    "    for idx, sentence in enumerate(data):\n",
    "        word_count = dict(Counter(sentence.split(' '))) #For each word get the count\n",
    "        for word, count in word_count.items():\n",
    "            if word not in stopwords.words('english'):\n",
    "                column_index = dict_word.get(word)\n",
    "                if column_index >= 0:\n",
    "                    row.append(idx)\n",
    "                    column.append(column_index)\n",
    "                    value.append(count)\n",
    "                    \n",
    "    result = csr_matrix((value, (row, column)), shape=(len(data), len(dict_word))) \n",
    "    result = result.toarray()\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0]\n",
      " [1 0 0 0 1]\n",
      " [0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#Sanity check\n",
    "print(CountVectorizer(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the count vectorizer works properly. \n",
    "The first sentence ('this is lab eleven') had lab and eleven (index 2 and 4) set as 1 because they appear once each, and the other word index set to 0 because they do not appear\n",
    "\n",
    "Reference:\n",
    "For more on this, please refer to <a href='https://medium.com/@saivenkat_/implementing-countvectorizer-from-scratch-in-python-exclusive-d6d8063ace22'>link<a/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement the count vectorization/tokenization\n",
    "X = CountVectorizer(corpus)\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEcg0XQxFFwl",
    "outputId": "d534a8a7-ace6-4cc0-da40-a349b312800b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15899"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview \n",
    "len(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9TwwJVEFr4p"
   },
   "source": [
    "Show that there are 15899 different words after stemming has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utjvtNInHWOs",
    "outputId": "5195bacf-7250-45a3-de46-feea6627ec07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1178"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnrxFhyEF_-g"
   },
   "source": [
    "- Term Frequency–inverse Document Frequency (Tf-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency–inverse Document Frequency  is basically a step further after tokenization/ bag-of-words model using count vectorization. It combines 2 concepts, Term Frequency (TF) and Document Frequency (DF). The term frequency is the number of occurrences of a specific term in a document. Term frequency indicates how important a specific term in a document. Inverse document frequency (IDF) is the weight of a term, it aims to reduce the weight of a term if the term’s occurrences are scattered throughout all the documents. The TF-IDF score as the name suggests is just a multiplication of the term frequency matrix with its IDF\n",
    "Reference:<a href='https://towardsdatascience.com/tf-idf-simplified-aba19d5f5530'> link <a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfIdf(data):\n",
    "    cv = CountVectorizer(data)\n",
    "    tf = cv / (np.sum(cv, axis=-1).reshape(-1,1))\n",
    "    idf = np.log((1 + len(cv)) / (1 + (cv!=0).sum(axis=0) )) + 1\n",
    "    tf_idf = tf * idf\n",
    "    #Normalize\n",
    "    tf_idf = tf_idf / np.linalg.norm(tf_idf, axis = -1).reshape(-1,1)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.70710678 0.         0.70710678 0.        ]\n",
      " [0.70710678 0.         0.         0.         0.70710678]\n",
      " [0.         0.         1.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Sanity check\n",
    "print(tfIdf(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the example above the tfIdf converts the previous count vectorized form of zeros and ones to a combination of term frequency and inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement the Term Frequency–inverse Document Frequency \n",
    "X1 = tfIdf(corpus)\n",
    "y1 = dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cUP8OXlIMDV"
   },
   "source": [
    "Splitting the dataset randomly into train / validation / test splits according to ratios 80%:10%:10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Bag-of-words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgI-WfzSF2tA",
    "outputId": "06b9485a-5664-499a-80c9-5bf1feac7cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set size:\n",
      "(942, 15899) (942,)\n",
      "Validation Set size:\n",
      "(118, 15899) (118,)\n",
      "Test set size:\n",
      "(118, 15899) (118,)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size =0.10, random_state =3116)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size =0.111, random_state =3116)\n",
    "print(\"Train Set size:\")\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(\"Validation Set size:\")\n",
    "print(X_val.shape,y_val.shape)\n",
    "print(\"Test set size:\")\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Term Frequency–inverse Document Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set size:\n",
      "(942, 15899) (942,)\n",
      "Validation Set size:\n",
      "(118, 15899) (118,)\n",
      "Test set size:\n",
      "(118, 15899) (118,)\n"
     ]
    }
   ],
   "source": [
    "X_train1,X_test1,y_train1,y_test1 = train_test_split(X1,y1,test_size =0.10, random_state =3116)\n",
    "X_train1,X_val1,y_train1,y_val1 = train_test_split(X_train1,y_train1,test_size =0.111, random_state =3116)\n",
    "print(\"Train Set size:\")\n",
    "print(X_train1.shape,y_train1.shape)\n",
    "print(\"Validation Set size:\")\n",
    "print(X_val1.shape,y_val1.shape)\n",
    "print(\"Test set size:\")\n",
    "print(X_test1.shape,y_test1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6ajm56VLJ27"
   },
   "source": [
    "#### Exercise 1: Implementing Naive Bayes Classifier for Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "m1VIYIdIXaDG"
   },
   "outputs": [],
   "source": [
    "class MultinomialNB():\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha \n",
    "        #for smoothing (Laplace)\n",
    "    def fit(self, X_train, y_train):\n",
    "        m, n = X_train.shape\n",
    "        self._classes = np.unique(y_train)\n",
    "        n_classes = len(self._classes)\n",
    "        self._priors = np.zeros(n_classes)\n",
    "        self._likelihoods = np.zeros((n_classes, n))\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_train_c = X_train[c == y_train]\n",
    "            self._priors[idx] = X_train_c.shape[0] / m  #Calculate the prior P(c)\n",
    "            self._likelihoods[idx, :] = ((X_train_c.sum(axis=0)) + self.alpha) / (np.sum(X_train_c.sum(axis=0) + self.alpha))\n",
    "            #Find the likelihood\n",
    "            #P(X|Y=c) = P(X|Y=c)/ P(Y=c)\n",
    "    def predict(self, X_test):\n",
    "        #Use the _predict helper function for each value\n",
    "        return [self._predict(x_test) for x_test in X_test]\n",
    "\n",
    "    def _predict(self, x_test):\n",
    "        posteriors = [] #posterior probability of assigning examples to each class\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            prior_class = np.log(self._priors[idx])\n",
    "            likelihoods_class = self.calc_likelihood(self._likelihoods[idx,:], x_test)\n",
    "            posteriors_class = np.sum(likelihoods_class) + prior_class\n",
    "            posteriors.append(posteriors_class)\n",
    "            #The class with the maximum posterior is selected as the predicted class\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def calc_likelihood(self, class_likelihood, x_test):\n",
    "        return np.log(class_likelihood) * x_test\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        return np.sum(y_pred == y_test)/len(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ep8N4orEo3iJ"
   },
   "source": [
    "Reference:<a href='https://stackoverflow.com/search?q=user:12312396+naivebayes&s=0e12d82b-e213-4061-808d-4074e67c52f3'> Link 1 <a/>  &nbsp; <a href='https://stackoverflow.com/questions/33830959/multinomial-naive-bayes-parameter-alpha-setting-scikit-learn'> Link 2 <a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0addVkP_aPSW",
    "outputId": "083e3a27-f14c-405b-bd94-977ab152b82d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53  0]\n",
      " [ 1 64]]\n"
     ]
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MP69yZ9qvZ_",
    "outputId": "834387d1-842f-46fc-f8fc-68e5cc061335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n"
     ]
    }
   ],
   "source": [
    "#Preview of second 20 test predictions comparing with actual categories\n",
    "for i,j in zip(y_test[20:40],y_pred[20:40]):\n",
    "    if i ==0:\n",
    "        i = \"sci.med\"\n",
    "    else:\n",
    "        i = \"comp.graphics\"\n",
    "    if j ==0:\n",
    "        j = \"sci.med\"\n",
    "    else:\n",
    "        j = \"comp.graphics\"\n",
    "    print(f'Actual category: {i}\\t Predicted category: {j}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpBUleeZrbsM"
   },
   "source": [
    "- Accuracy of Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwPKRdevoAI0",
    "outputId": "58636025-b8a9-47b6-8e79-d761c5c2a792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9915254237288136\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgwUr7_3ruqW"
   },
   "source": [
    "### Exercise 2: Implementing SVM Classifier via Scikit-Learn (bag-of-words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "duvP0yLepKOQ",
    "outputId": "9b4ebdeb-7036-400a-b360-8c27e079b94e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=SVC(random_state=3116),\n",
       "             param_grid={'C': [0.0, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5,\n",
       "                               0.6000000000000001, 0.7000000000000001, 0.8, 0.9,\n",
       "                               1.0, 1.1, 1.2000000000000002, 1.3,\n",
       "                               1.4000000000000001, 1.5, 1.6, 1.7000000000000002,\n",
       "                               1.8, 1.9000000000000001, 2.0, 2.1, 2.2,\n",
       "                               2.3000000000000003, 2.4000000000000004, 2.5, 2.6,\n",
       "                               2.7, 2.8000000000000003, 2.9000000000000004],\n",
       "                         'gamma': ['scale', 'auto'],\n",
       "                         'kernel': ['linear', 'rbf', 'poly', 'sigmoid']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(random_state=3116)\n",
    "svc = svc.fit(X_train,y_train) #model training\n",
    "#Hyperparameter tuning with the validation set\n",
    "grid_parameters = {'C':list(np.arange(0,3,0.1)),'kernel':['linear','rbf','poly','sigmoid'],'gamma':['scale','auto']}\n",
    "svc_search = GridSearchCV(svc, grid_parameters, cv=3,scoring='accuracy', verbose=0)\n",
    "svc_search.fit(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DNKpETovzjY4",
    "outputId": "b495b629-1990-48e8-a0f6-909b33f1b9a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 2.6, 'gamma': 'scale', 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", svc_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN1fJPOUoQu6"
   },
   "source": [
    "Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWDDZBEQoi8M",
    "outputId": "52e469f8-a871-4438-bc7e-71b38814bf9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9915254237288136\n",
      "[[53  0]\n",
      " [ 1 64]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\",accuracy_score(y_test,classifier.predict(X_test)))\n",
    "print(confusion_matrix(y_test,classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVN3Wv7polr1"
   },
   "source": [
    "SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GbdCo-_92-D0",
    "outputId": "6a8e3631-7008-47ba-a3e6-946f24eac635",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.8135593220338984\n",
      "[[47  6]\n",
      " [16 49]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\",accuracy_score(y_test,svc_search.predict(X_test)))\n",
    "print(confusion_matrix(y_test,svc_search.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c6jIF24gLg9"
   },
   "source": [
    "After tuning the hyperparameters with the validation set, we got an accuracy of 0.814 on the test set\n",
    "\n",
    "`N/b` - Tuning with the train set will get the model to perform with an accuracy of 1.0 on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency–inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53  0]\n",
      " [ 1 64]]\n"
     ]
    }
   ],
   "source": [
    "classifier1 = MultinomialNB()\n",
    "classifier1.fit(X_train1, y_train1)\n",
    "\n",
    "# Predicting test set\n",
    "y_pred1 = classifier1.predict(X_test1)\n",
    "y_test1 = np.array(y_test1)\n",
    "\n",
    "print(confusion_matrix(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n",
      "Actual category: sci.med\t Predicted category: sci.med\n",
      "Actual category: comp.graphics\t Predicted category: comp.graphics\n"
     ]
    }
   ],
   "source": [
    "#Preview of second 20 test predictions comparing with actual categories\n",
    "for i,j in zip(y_test1[20:40],y_pred1[20:40]):\n",
    "    if i ==0:\n",
    "        i = \"sci.med\"\n",
    "    else:\n",
    "        i = \"comp.graphics\"\n",
    "    if j ==0:\n",
    "        j = \"sci.med\"\n",
    "    else:\n",
    "        j = \"comp.graphics\"\n",
    "    print(f'Actual category: {i}\\t Predicted category: {j}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy of Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9915254237288136\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test1,y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implementing SVM Classifier via Scikit-Learn (TfIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=SVC(random_state=3116),\n",
       "             param_grid={'C': [0.0, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5,\n",
       "                               0.6000000000000001, 0.7000000000000001, 0.8, 0.9,\n",
       "                               1.0, 1.1, 1.2000000000000002, 1.3,\n",
       "                               1.4000000000000001, 1.5, 1.6, 1.7000000000000002,\n",
       "                               1.8, 1.9000000000000001, 2.0, 2.1, 2.2,\n",
       "                               2.3000000000000003, 2.4000000000000004, 2.5, 2.6,\n",
       "                               2.7, 2.8000000000000003, 2.9000000000000004],\n",
       "                         'gamma': ['scale', 'auto'],\n",
       "                         'kernel': ['linear', 'rbf', 'poly', 'sigmoid']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc1 = SVC(random_state=3116)\n",
    "svc1 = svc1.fit(X_train1,y_train1) #model training\n",
    "grid_parameters = {'C':list(np.arange(0,3,0.1)),'kernel':['linear','rbf','poly','sigmoid'],'gamma':['scale','auto']}\n",
    "svc_search1 = GridSearchCV(svc1, grid_parameters,scoring='accuracy', cv=3, verbose=0)\n",
    "svc_search1.fit(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 2.6, 'gamma': 'scale', 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", svc_search1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9915254237288136\n",
      "[[53  0]\n",
      " [ 1 64]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\",accuracy_score(y_test1,classifier1.predict(X_test1)))\n",
    "print(confusion_matrix(y_test1,classifier1.predict(X_test1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.4491525423728814\n",
      "[[53  0]\n",
      " [65  0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\",accuracy_score(y_test1,svc_search1.predict(X_test1)))\n",
    "print(confusion_matrix(y_test1,svc_search1.predict(X_test1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the hyperparameters with the validation set on the tfidf dataset, we got an accuracy of 0.449 on the test set.\n",
    "The Naive Bayes Classifier performs similar on both the bag-of-words embedding and the TfIdf embedding. However, the `SVM` performs poorly on the `TfIdf` embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Method|Feature Embedding|Test Accuracy|\n",
    "|-|-|-|\n",
    "|Naive Bayes Classifier|Bag-of-words| 0.9915|\n",
    "|-|-|-|\n",
    "|Naive Bayes Classifier|Tf-IDf| 0.9915|\n",
    "|-|-|-|\n",
    "|SVM Classifier|Bag-of-words|0.8136|\n",
    "|-|-|-|\n",
    "|SVM Classifier|Tf-IDf|0.4492|"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LCML11.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
